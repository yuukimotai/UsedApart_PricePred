{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情報量\n",
    "情報量はエントロピーとも呼ばれ、あるできごとがどれほど起こりにくいかを表す尺度です。  \n",
    "機械学習で、2つの確率分布の隔たりを表すのに使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 情報量とは？\n",
    "\n",
    "それぞれのできごとの情報量だけでなく、できごとの情報量の平均値も情報量と呼ぶことがあります。  \n",
    "前者を選択情報量（自己エントロピー）、後者を平均情報量（エントロピー）と呼びます。  \n",
    "\n",
    "情報量はあるできごとがどれほど起こりにくいかを表す尺度ですが、有用性を表す尺度ではありません。  \n",
    "例えば、1/100で当たるルーレットの賞金が1億円であっても0円であっても、情報量に違いはありません。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 選択情報量\n",
    "できごとEが起きた時の選択情報量は、次の式で表されます。\n",
    "\n",
    "$$  I(E) = -\\log_2 P(E) $$\n",
    "\n",
    "$P(E)$はEが起きる確率です。  \n",
    "このように、選択情報量は確率の対数として表されます。  \n",
    "対数の底には2を使うことが多いですが、底には何を選んでも本質的に違いはありません。\n",
    "\n",
    "上記の式を2の累乗の形で表すと次のようになります。\n",
    "\n",
    "$$ P(E) = \\frac{1}{2^{I(E)}} $$\n",
    "\n",
    "例えば両面が表の特殊なコインを投げる場合、「表の面が上になる」というできごとが起きる確率は1なので、選択情報量は$-\\log_2 1$で0となります。  \n",
    "通常の、片面が表で片面が裏のコインを投げる場合「表の面が上になる」というできごとが起きる確率は1/2なので、選択情報量は$-\\log_2 \\frac{1}{2}$で1となります。  \n",
    "\n",
    "このように、できごとの確率が小さくなるほど情報量は大きくなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 選択情報量をグラフ化\n",
    " \n",
    "横軸を確率、縦軸を選択情報量としたグラフを描画します。  \n",
    "底が2の対数は、NumPyのlog2関数で計算できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def entropy(x):\n",
    "    return -np.log2(x)  # 選択情報量\n",
    "\n",
    "x = np.linspace(0.01, 1)  # 0の対数はとれないので0.01に\n",
    "y = entropy(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確率の上昇に伴い選択情報量は単調に減少しています。  \n",
    "また、確率が0に近くなるとなると選択情報量が無限に増えていきます。  \n",
    "確率が1になると、選択情報量は0になります。  \n",
    "選択情報量が出来事の起こりにくさを表す尺度であることが分かります。\n",
    "\n",
    "また、選択情報量には「和をとれる」という性質があります。  \n",
    "トランプの例を考えてみましょう。  \n",
    "ジョーカー抜きの52枚から、スペードの1を引く確率は、1/52です。  \n",
    "従って、選択情報量は$-\\log_2 \\frac{1}{52} = \\log_2 52$となります。  \n",
    "このとき、$-\\log \\frac{1}{a} = \\log a$という関係を使っています。  \n",
    "\n",
    "また、スペードを引く確率は1/4なので、選択情報量は$-\\log_2 \\frac{1}{4} = \\log_2 4$となります。  \n",
    "1を引く確率は1/13なので、選択情報量は$-\\log_2 \\frac{1}{13} = \\log_2 13$となります。\n",
    "\n",
    "$\\log a + \\log b = \\log ab$の関係により、\n",
    "\n",
    "$$\\log_2 4 + \\log_2 13 = \\log_2 52$$\n",
    "\n",
    "となって、「スペードを引く」の選択情報量と「1を引く」の選択情報量の和は、「スペードの1を引く」の選択情報量と等しくなります。  \n",
    "このように、選択情報量には和をとれるという便利な性質があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平均情報量\n",
    "平均情報量は、次の式で定義されます。\n",
    "\n",
    "$$H = -\\sum_{k=1}^n P(E_k)\\log_2 P(E_k)$$\n",
    "\n",
    "$n$はできごとの総数で、$E_k$は各できごとを表します。  \n",
    "選択情報量に、確率をかけて総和をとったものになっています。  \n",
    "単にエントロピーという場合は、この平均情報量を指すことが多いです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平均情報量の意味\n",
    "\n",
    "コイン投げの例を考えましょう。  \n",
    "あるコインの表が出る確率がP、裏が出る確率を1-Pとします。  \n",
    "このとき、平均情報量は次のように求めることができます。\n",
    "\n",
    "$$ H = -P\\log_2 P - (1-P)\\log_2 (1-P) $$\n",
    "\n",
    "これをグラフで描画しましょう。  \n",
    "以下のコードは、横軸を確率、縦軸を平均情報量としてグラフを描画します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def entropy(x):\n",
    "    return -np.log2(x)  # 選択情報量\n",
    "\n",
    "x = np.linspace(0.01, 0.99)  # 0の対数はとれないので0.01-0.99に\n",
    "y = x*entropy(x) + (1-x)*entropy(1-x)  # 平均情報量\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均情報量はPが0と1のとき0になり、0.5で最大値の1になります。  \n",
    "平均情報量は、結果の予測がしにくいときに大きく、予測がしやすいときに小さくなります。  \n",
    "すなわち、あるできごとの発生確率がすべて同じとき、すなわち何が起こるか予測がつかないときに最大になります。  \n",
    "\n",
    "また、発生確率の偏りが大きいほど平均情報量は小さくなる、と表現することもできます。  \n",
    "平均情報量は、情報の無秩序さや不確実さを表す尺度でもあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演習:\n",
    "以下のセルを補完し、表が上になる確率が0.6、裏が上になる確率が0.4のコインを投げることの平均情報量を求めましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(x):\n",
    "    return -np.log2(x)  # 選択情報量\n",
    "\n",
    "p = 0.6\n",
    "print()  # 平均情報量を求める"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
